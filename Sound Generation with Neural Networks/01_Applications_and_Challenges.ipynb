{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications and Challenges\n",
    "\n",
    "### The sound generation task can be broken down in 2 steps:\n",
    "- 1: Training a model with audio\n",
    "- 2: Generating audio with the trained model with an input\n",
    "\n",
    "### What kind of input do we feed the model? There are mainly 2 possibilities:\n",
    "1. Raw Audio: the actual digitized audio\n",
    "    - too many datapoints (~44.1k samples / second)\n",
    "    - it's hard to figure out long-range dependencies\n",
    "    - computationally expensive, because of the amount of samples\n",
    "    \n",
    "2. Spectograms: high-level representation of sound using time and frequency. similar to an image.\n",
    "    - needs preprocessing (raw audio -> spectogram (Short-time Fourier Transform))\n",
    "    - needs postprocessing (spectogram -> raw audio (Inverse Short-time Fourier Transform))\n",
    "    - can capture long-range dependencies\n",
    "    - worst audio fidelity\n",
    "\n",
    "### Deep Learning Architectures for Sound Generations\n",
    "- GAN\n",
    "- Autoencoders\n",
    "- Variational Autoencoders (VAE)\n",
    "- VQ-VAE\n",
    "\n",
    "### Inputs for the Trained Model (Generation Phase)\n",
    "- Conditioned Generation: pass information about the song (genre, artist, ...)\n",
    "- Autonomous Generation: generate sound without any input\n",
    "- Continuation: pass a 'seed' (note, short audio) and let the network continue\n",
    "\n",
    "### For this course, we'll use\n",
    "- Spectograms\n",
    "- Variational Autoencoders\n",
    "- Autonomous Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
