{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt', 'r') as f:\n",
    "    names = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = {}\n",
    "for name in names:\n",
    "    aug_name = ['.'] + list(name) + ['.']\n",
    "    for char_1, char_2 in zip(aug_name, aug_name[1:]):\n",
    "        chars = (char_1, char_2)\n",
    "        bigrams[chars] = bigrams.get(chars, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "possible_chars = set(''.join(names))\n",
    "possible_chars.add('.')\n",
    "\n",
    "n_possible_chars = len(possible_chars)\n",
    "\n",
    "char_to_index = {char: i for i, char in enumerate(sorted(possible_chars))}\n",
    "index_to_char = {index: char for char, index in char_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tensor = torch.zeros((n_possible_chars, n_possible_chars))\n",
    "\n",
    "for key, value in bigrams.items():\n",
    "    char_1 = key[0]\n",
    "    char_2 = key[1]\n",
    "\n",
    "    index_1 = char_to_index[char_1] # row = character start\n",
    "    index_2 = char_to_index[char_2] # col = character end\n",
    "    bigram_tensor[index_1, index_2] = value\n",
    "\n",
    "bigram_tensor = bigram_tensor.float()\n",
    "bigram_probabilities = bigram_tensor/bigram_tensor.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".sigausssarahue.\n",
      ".a.\n",
      ".ali.\n",
      ".suoynnaiartahowir.\n",
      ".jakeaynin.\n",
      ".ritadr.\n",
      ".sant.\n",
      ".nng.\n",
      ".a.\n",
      ".heeio.\n"
     ]
    }
   ],
   "source": [
    "# Generate Words using Bigram Information\n",
    "\n",
    "n_words = 10\n",
    "for _ in range(n_words):\n",
    "    c = '.'\n",
    "    word = '.'\n",
    "    while True:\n",
    "        c_index = char_to_index[c] # get the index representation of the character\n",
    "        probabilities = bigram_probabilities[c_index] # get the prob vector of the index's row\n",
    "        new_index = torch.multinomial(probabilities, replacement=True, num_samples=1).item() # sample once\n",
    "        c = index_to_char[new_index] # get char from sampled index\n",
    "        word += c # add it to word\n",
    "        if c == '.': # '.' represents the end of the wod\n",
    "            break\n",
    "    print(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".yrnijytrjodvaculxerhhkdtwfdjlzlbfuucbgrpqyghpibvfxntqsofnsaxxa.\n",
      ".cvaduyjuerhffsrnzcmdqvpkrayxzckmsca.\n",
      ".hezprkppewceuhcyqsareuydlommrclaki.\n",
      ".zpumjiguhlazzadt.\n",
      ".ogdcpigg.\n",
      ".tcnzqhwsfmpiqswwqrp.\n",
      ".kqcdokknak.\n",
      "..\n",
      ".uapmxxqn.\n",
      ".fdbof.\n"
     ]
    }
   ],
   "source": [
    "# Generate Words using Uniform Distribution\n",
    "\n",
    "uniform_tensor = torch.ones(n_possible_chars, n_possible_chars)\n",
    "probabilities_uniform = uniform_tensor/uniform_tensor.sum(1, keepdim=True)\n",
    "\n",
    "n_words = 10\n",
    "for _ in range(n_words):\n",
    "    c = '.'\n",
    "    word = '.'\n",
    "    while True:\n",
    "        c_index = char_to_index[c]\n",
    "        probabilities = probabilities_uniform[c_index] # get the prob vector of the index's row\n",
    "        new_index = torch.multinomial(probabilities, replacement=True, num_samples=1).item()\n",
    "        c = index_to_char[new_index]\n",
    "        word += c\n",
    "        if c == '.':\n",
    "            break\n",
    "    print(word)\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our 'model'\n",
    "\n",
    "If the model is performing well, then P(bigram) for bigrams in our data ste should be high.\n",
    "\n",
    "Here, I'll be using the sum of -log(P) because, since probabilities are small, their multiplications will often lead to underflow. That is, it will fail to represent very small numbers accurately.\n",
    "\n",
    "The negative sign is used to make it a proper 'loss' function. Since  log(P) <= 0 for P in [0,1], -log(P) makes the number positive, with the lowest possible loss function being 0, when P = 1 for all bigrams in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4541)\n"
     ]
    }
   ],
   "source": [
    "# 'Learned' Probabilities\n",
    "\n",
    "p_log_sum = 0\n",
    "n_bigrams = 0\n",
    "for name in names:\n",
    "    name_mod = '.' + name + '.'\n",
    "    for char1, char2 in zip(name_mod, name_mod[1:]):\n",
    "        idx1 = char_to_index[char1]\n",
    "        idx2 = char_to_index[char2]\n",
    "        p = bigram_probabilities[idx1, idx2]\n",
    "        p_log = torch.log(p)\n",
    "        p_log_sum += p_log\n",
    "        n_bigrams += 1\n",
    "\n",
    "neg_p_log_avg = -p_log_sum/n_bigrams\n",
    "print(neg_p_log_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2960)\n"
     ]
    }
   ],
   "source": [
    "# Uniform Probabilities \n",
    "\n",
    "p_log_sum = 0\n",
    "n_bigrams = 0\n",
    "for name in names:\n",
    "    name_mod = '.' + name + '.'\n",
    "    for char1, char2 in zip(name_mod, name_mod[1:]):\n",
    "        idx1 = char_to_index[char1]\n",
    "        idx2 = char_to_index[char2]\n",
    "        p = probabilities_uniform[idx1, idx2]\n",
    "        p_log = torch.log(p)\n",
    "        p_log_sum += p_log\n",
    "        n_bigrams += 1\n",
    "\n",
    "neg_p_log_avg = -p_log_sum/n_bigrams\n",
    "print(neg_p_log_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5 |Anaconda custom (64-bit)| (default, Apr 26 2018, 08:42:37) \n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4855804758c501db6aeaadaa938f94774f80469bfc4937e2dd123e1f15b80cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
